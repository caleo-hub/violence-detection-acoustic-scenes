{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não há diferença significativa entre os modelos em relação a acurácia.\n"
     ]
    }
   ],
   "source": [
    "acuracia_1 = np.array([0.848, 0.830,0.842,0.863,0.863,0.849])\n",
    "acuracia_2 = np.array([0.850, 0.831,0.845, 0.847, 0.831, 0.841])\n",
    "\n",
    "statistic, p_value = ttest_rel(acuracia_1, acuracia_2)\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\n",
    "        f\"As acurácias apresentam uma diferença significativa entre os modelos.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Não há diferença significativa entre os modelos em relação a acurácia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_classe0 0.4026587880360759\n",
      "recall_classe0 0.21809846848806713\n",
      "f1-score_classe0 0.21400866188055903\n",
      "precision_classe1 0.8559330705559979\n",
      "recall_classe1 0.15364283190760133\n",
      "f1-score_classe1 0.011883227132229307\n",
      "precision_classe2 0.30049950299585504\n",
      "recall_classe2 0.6755636710071817\n",
      "f1-score_classe2 0.41924174292325683\n",
      "precision_classe3 0.4402145643816966\n",
      "recall_classe3 0.5820207731323164\n",
      "f1-score_classe3 0.07431919071447485\n",
      "A métrica 'f1-score_classe1' apresenta uma diferença significativa entre os modelos.\n"
     ]
    }
   ],
   "source": [
    "def perform_ttest(csv_file1, csv_file2):\n",
    "    # Leitura dos arquivos CSV\n",
    "    df1 = pd.read_csv(csv_file1)\n",
    "    df2 = pd.read_csv(csv_file2)\n",
    "\n",
    "    ttest_results = {}\n",
    "    for (index_1, row_1), (index_2, row_2) in zip(df1.iterrows(), df2.iterrows()):\n",
    "        \n",
    "        assert row_1[\"Metrica\"] == row_2[\"Metrica\"]\n",
    "        data1 = [\n",
    "            row_1[\"Fold1\"],\n",
    "            row_1[\"Fold2\"],\n",
    "            row_1[\"Fold3\"],\n",
    "            row_1[\"Fold4\"],\n",
    "            row_1[\"Fold5\"],\n",
    "        ]\n",
    "        data2 = [\n",
    "            row_2[\"Fold1\"],\n",
    "            row_2[\"Fold2\"],\n",
    "            row_2[\"Fold3\"],\n",
    "            row_2[\"Fold4\"],\n",
    "            row_2[\"Fold5\"],\n",
    "        ]\n",
    "\n",
    "        # Ignora a métrica 'support'\n",
    "        if \"support\" in row_1[\"Metrica\"]:\n",
    "            continue\n",
    "\n",
    "        # Executa o teste T pareado\n",
    "        statistic, p_value = ttest_rel(data1, data2)\n",
    "        # Armazena o resultado do teste T\n",
    "        ttest_results[row_1[\"Metrica\"]] = p_value\n",
    "\n",
    "    return ttest_results\n",
    "\n",
    "\n",
    "def compare_models(csv_file1, csv_file2):\n",
    "    # Chamar a função perform_ttest para obter os resultados do teste T\n",
    "    ttest_results = perform_ttest(csv_file1, csv_file2)\n",
    "\n",
    "    # Verificar o valor de p para cada métrica e identificar a melhor métrica ou se não há diferença significativa\n",
    "    best_metric = None\n",
    "    for metric, p_value in ttest_results.items():\n",
    "        print(metric, p_value)\n",
    "        if p_value < 0.05:\n",
    "            if best_metric is None or p_value < ttest_results[best_metric]:\n",
    "                best_metric = metric\n",
    "\n",
    "    if best_metric is not None:\n",
    "        print(\n",
    "            f\"A métrica '{best_metric}' apresenta uma diferença significativa entre os modelos.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Não há diferença significativa entre os modelos.\")\n",
    "\n",
    "\n",
    "# Exemplo de uso\n",
    "csv_file1 = \"metrics_mobilenet.csv\"\n",
    "csv_file2 = \"metrics_resnet152.csv\"\n",
    "\n",
    "compare_models(csv_file1, csv_file2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
