{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_preprocessing.audio_dataset_integration import AudioDatasetIntegrator\n",
    "\n",
    "\n",
    "def audio_integrator():\n",
    "    datasets_path = [\n",
    "        (\"Dataset_teste\", \"../Dataset_teste\"),\n",
    "    ]\n",
    "\n",
    "    for dataset_name, dataset_path in datasets_path:\n",
    "        output_path = f\"h5_files/{dataset_name}_dataset.h5\"\n",
    "        print(f\"Processing {dataset_name} dataset\")\n",
    "        integrator = AudioDatasetIntegrator(dataset_path, output_path)\n",
    "        integrator.process_audio_files()\n",
    "        integrator.annotate_audio_files()\n",
    "        print(f\"{dataset_name} dataset processed and annotated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_preprocessing.audio_feature_extractor import AudioFeatureExtractor\n",
    "\n",
    "\n",
    "def audio_extractor():\n",
    "    datasets_path = [\n",
    "        (\"Dataset_teste\", \"h5_files/Dataset_teste_dataset.h5\"),\n",
    "    ]\n",
    "\n",
    "    for dataset_name, dataset_path in datasets_path:\n",
    "        output_path = f\"h5_files/{dataset_name}_feature.h5\"\n",
    "        print(f\"Processing {dataset_name} dataset\")\n",
    "        feature_extractor = AudioFeatureExtractor(dataset_path, output_path)\n",
    "        feature_extractor.extract_features(n_fft=640)\n",
    "        print(f\"{dataset_name} dataset processed and annotated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gravar_audio(path, duracao):\n",
    "    # Configuração do dispositivo de áudio\n",
    "    samplerate = 44100  # Taxa de amostragem em Hz\n",
    "    channels = 1       # Número de canais de áudio (estéreo)\n",
    "\n",
    "    # Gravação do áudio\n",
    "    print(f\"Gravando áudio por {duracao} segundos...\")\n",
    "    gravacao = sd.rec(int(duracao * samplerate), samplerate=samplerate, channels=channels)\n",
    "    sd.wait()  # Aguarda a gravação ser concluída\n",
    "\n",
    "    # Salvando o arquivo de áudio\n",
    "    sf.write(path, gravacao, samplerate)\n",
    "\n",
    "    print(f\"Áudio gravado e salvo em: {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravando áudio por 10 segundos...\n",
      "Áudio gravado e salvo em: ../Dataset_teste/teste.wav\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de uso\n",
    "caminho_arquivo = \"../Dataset_teste/teste.wav\"\n",
    "duracao_gravacao = 10  # Duração em segundos\n",
    "\n",
    "gravar_audio(caminho_arquivo, duracao_gravacao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset_teste dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Walking through directories: 1it [00:02,  2.10s/it]\n",
      "Annotating audio files: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_teste dataset processed and annotated\n",
      "Processing Dataset_teste dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1/1 [00:00<00:00, 13.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_teste dataset processed and annotated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "audio_integrator()\n",
    "audio_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 251, 29])\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "file_path = \"h5_files/Dataset_teste_feature.h5\"\n",
    "with h5py.File(file_path, \"r\") as hf:\n",
    "    inputs = hf[\"features\"]['0'][()]\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float).reshape(1,1,251,29)\n",
    "    print(inputs.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Final prediction: Nada\n",
      "Votes for class Nada: 2\n",
      "Votes for class Grito: 2\n",
      "Votes for class Violência Física: 0\n",
      "Votes for class Tiro: 1\n"
     ]
    }
   ],
   "source": [
    "annotation_dict = {\n",
    "    0: \"Nada\",\n",
    "    1: \"Grito\",\n",
    "    2: \"Violência Física\",\n",
    "    3: \"Tiro\",\n",
    "\n",
    "}\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model_folder_path = \"models/modelos_finais\"\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = models.resnet101()\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)  # ajustar a última camada para 6 classes\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )  # ajustar a primeira camada para 1 canal\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "models_list = [load_model(os.path.join(model_folder_path, model_name)) for model_name in os.listdir(model_folder_path) if model_name.endswith('.pth')]\n",
    "\n",
    "preds_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    for model in models_list:\n",
    "        outputs = model(inputs)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, preds = torch.max(probabilities, 1)\n",
    "        preds_all.append(preds.cpu().numpy()[0])\n",
    "    \n",
    "    final_pred = max(set(preds_all), key = preds_all.count) # class with max votes\n",
    "\n",
    "    print('Final prediction:', annotation_dict[final_pred])\n",
    "\n",
    "    # Count of votes for each class\n",
    "    count_votes = Counter(preds_all)\n",
    "\n",
    "    for class_index in annotation_dict:\n",
    "        print(f\"Votes for class {annotation_dict[class_index]}: {count_votes[class_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
